{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["ykBk_DEpItgo"],"mount_file_id":"1ASb6N5tISBl_AW2IsRV0ge10ei0gpZto","authorship_tag":"ABX9TyNn5kF3+cg6lnGj+/p1WtkF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Preparation for Visualization"],"metadata":{"id":"foHOTNOdJZGs"}},{"cell_type":"markdown","source":["## Model Loading and Environment Configuration"],"metadata":{"id":"ykBk_DEpItgo"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from pathlib import Path\n","BASE_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/3학년 2학기/컴퓨터비전/TEAM_5\")\n","%cd {BASE_DIR}\n","\n","import numpy as np\n","import gradio as gr\n","import cv2, sys, torch\n","from functools import partial\n","import matplotlib.pyplot as plt\n","\n","from src.utils.image_io import denorm_to_uint8\n","from src.dataprep.transforms import NormalMapToTensor\n","from src.autoencoder.inference import load_model\n","\n","print('-'*126)\n","print(f\"Python Version: {sys.version}\")\n","print(f\"PyTorch Version: {torch.__version__}\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","print('-'*126)\n","model_path = 'checkpoints/autoencoder/best_model_epoch_100.pth'\n","model, device = load_model(model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLEpIjJFMiXo","executionInfo":{"status":"ok","timestamp":1764832765796,"user_tz":-540,"elapsed":67463,"user":{"displayName":"김서현","userId":"08818282308943516715"}},"outputId":"271edfb5-a442-4a8b-ea49-349536c0f78b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/3학년 2학기/컴퓨터비전/TEAM_5\n","------------------------------------------------------------------------------------------------------------------------------\n","Python Version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n","PyTorch Version: 2.9.0+cu126\n","Device: cpu\n","------------------------------------------------------------------------------------------------------------------------------\n","[load_model] 모델 로드 완료: checkpoints/autoencoder/best_model_epoch_100.pth\n","[load_model] device: cpu\n"]}]},{"cell_type":"markdown","source":["## Function Definitions"],"metadata":{"id":"ZqLEZq0zIvOE"}},{"cell_type":"code","source":["# The input size used by the Dataset\n","TARGET_SIZE = (256, 256)\n","\n","GLOBAL_VMIN = 0.0\n","GLOBAL_VMAX = 0.10\n","\n","# Transform instance identical to the one used in the Dataset (resize / interpolation)\n","_normal_transform = NormalMapToTensor(size=TARGET_SIZE)\n","\n","\n","# ---------------------------------------------------------\n","# 1. Preprocessing identical to the Dataset pipeline\n","# ---------------------------------------------------------\n","def preprocess_np_for_model(image_np):\n","    \"\"\"\n","    image_np: (H, W, 3), uint8 in [0, 255]  (image received from Gradio)\n","\n","    Dataset pipeline:\n","        Load .npy → [-1, 1], (H, W, 3) → NormalMapToTensor → (3, H, W), [-1, 1]\n","\n","    Here, since the input is PNG:\n","        [0,255] → [0,1] → [-1,1],\n","    followed by passing through the same NormalMapToTensor transform.\n","    \"\"\"\n","    # [0,255] → [0,1] → [-1,1]\n","    img = image_np.astype(np.float32) / 255.0\n","    img = img * 2.0 - 1.0  # Same scale as the normal_np used in the Dataset\n","\n","    # Apply the same transform as in the Dataset (includes resizing)\n","    normal_tensor = _normal_transform(img)  # (3,H,W), [-1,1]\n","\n","    return normal_tensor  # torch.Tensor\n","\n","\n","# ---------------------------------------------------------\n","# 2. Error-map computation identical to evaluator.py\n","# ---------------------------------------------------------\n","def forward_and_error(model, input_tensor, device):\n","    \"\"\"\n","    input_tensor: (3,H,W), [-1,1]\n","    return:\n","      recon_np : (H,W,3) float [-1,1]\n","      heatmap  : (H,W)   float  (pixel-wise mean absolute error)\n","    \"\"\"\n","    model.eval()\n","\n","    with torch.no_grad():\n","        if input_tensor.dim() == 3:\n","            input_tensor = input_tensor.unsqueeze(0).to(device)  # (1,3,H,W)\n","        else:\n","            input_tensor = input_tensor.to(device)\n","\n","        recon_tensor = model(input_tensor)\n","\n","        # Convert tensors back to numpy (same as evaluator.py)\n","        input_np = (\n","            input_tensor[0]\n","            .detach()\n","            .cpu()\n","            .permute(1, 2, 0)\n","            .numpy()\n","        )  # (H,W,3), [-1,1]\n","\n","        recon_np = (\n","            recon_tensor[0]\n","            .detach()\n","            .cpu()\n","            .permute(1, 2, 0)\n","            .numpy()\n","        )  # (H,W,3), [-1,1]\n","\n","        # Error map computation (identical to evaluator.py)\n","        error_map_rgb = np.abs(input_np - recon_np)  # (H,W,3)\n","        heatmap = error_map_rgb.mean(axis=2)         # (H,W)\n","\n","    return recon_np, heatmap\n","\n","\n","# ---------------------------------------------------------\n","# 3. Wrapper for visualization\n","# ---------------------------------------------------------\n","def infer(model, input_tensor, device):\n","    \"\"\"\n","    input_tensor: (3, H, W), [-1,1]\n","    return:\n","        - recon_vis: (H,W,3), uint8 [0,255]  (for visualization)\n","        - heat_vis : (H,W,3), uint8 [0,255]  (colormap visualization)\n","        - heatmap  : (H,W), float (raw error values)\n","    \"\"\"\n","    # 1) Forward pass + error-map computation (same as evaluate_model)\n","    recon_np, heatmap = forward_and_error(model, input_tensor, device)\n","\n","    # 2) Convert reconstructed output into uint8 for visualization\n","    recon_vis = denorm_to_uint8(recon_np)  # (H,W,3) uint8\n","\n","    # 3) Convert heatmap into a visual colormap (similar to evaluator’s cv2.normalize routine)\n","    h_min, h_max = GLOBAL_VMIN, GLOBAL_VMAX\n","    if h_max > h_min:\n","        heat_norm = (heatmap - h_min) / (h_max - h_min)\n","    else:\n","        heat_norm = np.zeros_like(heatmap)\n","\n","    heat_uint8 = (heat_norm * 255).astype(np.uint8)\n","    heat_color = cv2.applyColorMap(heat_uint8, cv2.COLORMAP_JET)\n","    heat_vis = cv2.cvtColor(heat_color, cv2.COLOR_BGR2RGB)\n","\n","    # Upsample for Gradio UI display (does not affect numerical values)\n","    recon_vis = cv2.resize(recon_vis, (512, 512), interpolation=cv2.INTER_LINEAR)\n","    heat_vis = cv2.resize(heat_vis, (512, 512), interpolation=cv2.INTER_NEAREST)\n","\n","    return recon_vis, heat_vis, heatmap\n","\n","\n","# ---------------------------------------------------------\n","# 4. Function called by Gradio\n","# ---------------------------------------------------------\n","def infer_gradio(model, device, image_np):\n","    \"\"\"\n","    Gradio wrapper.\n","      - image_np: (H,W,3) uint8  [0,255]\n","      - return: (recon_vis, matplotlib Figure)\n","    \"\"\"\n","    # 1) Preprocess input identically to the Dataset pipeline\n","    input_tensor = preprocess_np_for_model(image_np)  # (3,H,W), [-1,1]\n","\n","    # 2) Forward pass + error-map computation\n","    recon_vis, _, heatmap = infer(model, input_tensor, device)\n","\n","    # 3) Create matplotlib figure\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(\n","        heatmap,\n","        cmap=\"jet\",\n","        vmin=GLOBAL_VMIN,\n","        vmax=GLOBAL_VMAX,\n","    )\n","    plt.colorbar(im, ax=ax)\n","    ax.set_title(\"Error Map\")\n","    ax.axis(\"off\")\n","\n","    return recon_vis, fig\n"],"metadata":{"id":"5TSVLkAbIOTL","executionInfo":{"status":"ok","timestamp":1764834267631,"user_tz":-540,"elapsed":123,"user":{"displayName":"김서현","userId":"08818282308943516715"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# UI Construction and Execution"],"metadata":{"id":"7CiMwjfmJDFs"}},{"cell_type":"code","source":["\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## AutoEncoder-based Anomaly Detection Demo\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=2):\n","            input_img = gr.Image(type=\"numpy\", label=\"Input\", width=512, height=512)\n","        with gr.Column(scale=2):\n","            gr.HTML(\"<div style='height:5px'></div>\")\n","            recon_img = gr.Image(label=\"Reconstructed\", width=512, height=412)\n","        with gr.Column(scale=2):\n","            gr.HTML(\"<div style='height:30px'></div>\")\n","            error_plot = gr.Plot(label=\"Error Map\")\n","\n","    with gr.Row():\n","        clear_btn = gr.Button(\"Clear\", scale=1)\n","        submit_btn = gr.Button(\"Submit\", scale=1, variant=\"primary\")\n","\n","    infer_fn = partial(infer_gradio, model, device)\n","\n","    submit_btn.click(\n","        fn=infer_fn,\n","        inputs=input_img,\n","        outputs=[recon_img, error_plot],\n","    )\n","\n","    clear_btn.click(\n","        fn=lambda: (None, None, None),\n","        inputs=None,\n","        outputs=[input_img, recon_img, error_plot],\n","    )\n","\n","demo.launch(\n","    share=True,\n","    debug=False,\n","    quiet=True,\n","    inline=False,\n","    inbrowser=True,\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6PFCdtJzKh3","executionInfo":{"status":"ok","timestamp":1764834272310,"user_tz":-540,"elapsed":710,"user":{"displayName":"김서현","userId":"08818282308943516715"}},"outputId":"2e4497be-94b1-4289-8332-7f83bc980ecb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["* Running on public URL: https://d4f3a6041b6522aaa1.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":11}]}]}